{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 python-docx\n",
        "!pip install PyMuPDF\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URI9LIFdAk6s",
        "outputId": "40128c8c-f8a1-438e-8a36-06c64c715d5d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.7)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.7 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from docx import Document\n",
        "import glob\n",
        "import os\n",
        "import fitz\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "TUl4j-uOBTUm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(file_path):\n",
        "    with fitz.open(file_path) as pdf_document:\n",
        "        text = ''\n",
        "        for page_number in range(pdf_document.page_count):\n",
        "            page = pdf_document[page_number]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    text = ''\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + '\\n'\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ec3NWABG-pMj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = re.sub(url_pattern, '', text)\n",
        "    text = re.sub(r'[^\\w\\s/+â€”-]', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "lacMw5r5Krbp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for files in glob.iglob('/content/*'):\n",
        "   if files.split('.')[-1]=='pdf':\n",
        "      text=read_pdf(files)\n",
        "   elif files.split('.')[-1]=='docx' or files.split('.')[-1]=='docs':\n",
        "      text=read_docx(files)\n",
        "\n"
      ],
      "metadata": {
        "id": "-TYIt3lEBmO3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emails(text):\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    return re.findall(email_pattern, text)\n"
      ],
      "metadata": {
        "id": "SRYbgUGNMpVu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_headings(text):\n",
        "    predefined_headings = [\n",
        "        \"PROFESSIONAL EXPERIENCE\",\n",
        "        \"TECH STACK\",\n",
        "        \"Education\"\n",
        "    ]\n",
        "\n",
        "    pattern = rf'^({\"|\".join(map(re.escape, predefined_headings))})\\n'\n",
        "    matches = re.finditer(pattern, text, flags=re.MULTILINE)\n",
        "\n",
        "    heading_start_indexes = [match.start() for match in matches]\n",
        "    return heading_start_indexes\n"
      ],
      "metadata": {
        "id": "1Yctqk0yOaCz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_resume_by_headings(text, heading_start_indexes):\n",
        "\n",
        "    heading_end_indexes = heading_start_indexes[1:] + [len(text)]\n",
        "    resume_parts = [text[start:end].strip() for start, end in zip(heading_start_indexes, heading_end_indexes)]\n",
        "\n",
        "    return resume_parts"
      ],
      "metadata": {
        "id": "KlWHDjd4W7-7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_recognition(text):\n",
        "  token_classification_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",aggregation_strategy=\"simple\")\n",
        "  predictions = token_classification_pipeline(text)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "KDB9IAEdipkL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_list_of_dicts(skills_string):\n",
        "    skills_list = skills_string.split(',')\n",
        "    skills_dicts = [{\"skills\": skill.strip()} for skill in skills_list]\n",
        "    return skills_dicts"
      ],
      "metadata": {
        "id": "mQdMrhBr4UjS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "List_of_split_text=split_resume_by_headings(text, extract_headings(text))\n",
        "List_of_split_text.insert(0,text[:extract_headings(text)[0]])\n",
        "indian_state_short_forms = [\"MH\", \"KA\", \"TN\", \"UP\", \"MP\", \"RJ\", \"WB\", \"AP\", \"TG\", \"KL\", \"GJ\", \"DL\", \"BR\", \"HR\", \"AS\", \"OR\", \"JH\", \"UK\", \"CT\", \"HP\", \"JK\", \"TR\", \"ML\", \"NL\", \"MN\", \"AR\", \"MZ\", \"SK\", \"PB\", \"GA\", \"WB\"]\n",
        "country_short_forms = [\"USA\", \"UK\", \"IN\", \"AUS\", \"CAN\", \"GER\", \"FRA\", \"CHN\", \"JPN\", \"BRA\"]\n"
      ],
      "metadata": {
        "id": "KdCGD06KXZJ_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_resume={'address':{\"city\": \"\",\"country\": \"\",\"state\": \"\"},\"education_history\":'',\"email\":\"\",\n",
        "    \"first_name\": \"\",\n",
        "    \"last_name\": \"\",\n",
        "    \"phone\": \"\",\n",
        "    \"skills\":\"\",\"summary\":\"\",\"work_history\":\"\"}"
      ],
      "metadata": {
        "id": "Ec9tMj9EanTl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "work_history=[]\n",
        "skills=[]\n",
        "education_history=[]\n",
        "date_pattern = r'([A-Za-z]+) (\\d{4})'"
      ],
      "metadata": {
        "id": "CAT0gkyBxSVb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(List_of_split_text)):\n",
        "    text = List_of_split_text[j]\n",
        "    if j==0:\n",
        "      parsed_resume['email']=extract_emails(text)\n",
        "      phone_sindex=text.find('+91')\n",
        "      parsed_resume['phone']=text[phone_sindex-1:phone_sindex+13]\n",
        "      entity=entity_recognition(text)\n",
        "      for i in entity:\n",
        "        if i['entity_group']=='PER':\n",
        "           Name=i['word']\n",
        "           if len(Name.split(' '))>1:\n",
        "               parsed_resume['first_name']=Name.split(' ')[0]\n",
        "               parsed_resume['last_name']=Name.split(' ')[1:]\n",
        "        if i['entity_group']=='LOC':\n",
        "           if i['word'] in indian_state_short_forms:\n",
        "               parsed_resume['address']['state']=i['word']\n",
        "           elif i['word'] in country_short_forms:\n",
        "               parsed_resume['address']['country']=i['word']\n",
        "           else:\n",
        "               parsed_resume['address']['city']=i['word']\n",
        "\n",
        "      parsed_resume['summary']=text[i['end']:]\n",
        "    elif 'TECH STACK' in text.splitlines():\n",
        "        skills=','.join(text.splitlines()[1:])\n",
        "        parsed_resume['skills']=convert_to_list_of_dicts(skills)\n",
        "\n",
        "    elif 'Education' in text.splitlines():\n",
        "      text=text.splitlines()[1:]\n",
        "      education_history.append({\"degree\":text[0],\"from_date\":text[-1],'name':text[1],'end_date':''})\n",
        "      parsed_resume['education_history']=education_history\n",
        "\n",
        "    elif 'PROFESSIONAL EXPERIENCE' in text.splitlines():\n",
        "      work_history=[]\n",
        "      text=clean_text(text).splitlines()[1:]\n",
        "      filtered_list = list(filter(None, text))\n",
        "      Org=[]\n",
        "      for org in filtered_list:\n",
        "        if len(org.split(' '))<=3:\n",
        "           e=entity_recognition(org)\n",
        "           if len(e)>0:\n",
        "              if e[0]['entity_group']=='ORG':\n",
        "                  Org.append(org)\n",
        "      i=0\n",
        "      while i<len(Org)-1:\n",
        "         start_index=filtered_list.index(Org[i])\n",
        "         end_index=filtered_list.index(Org[i+1])\n",
        "         j=[]\n",
        "         new_list=filtered_list[start_index:end_index+1]\n",
        "         j.append({'Company':new_list[0],'Designation':new_list[1],'Joining Date':new_list[3],'Description':' '.join(new_list[4:])})\n",
        "         work_history.append(j)\n",
        "         i+=1\n",
        "      last_company_info = filtered_list[filtered_list.index(Org[-1]):]\n",
        "      last_work_entry = {\n",
        "          'Company': last_company_info[0],\n",
        "          'Designation': last_company_info[1],\n",
        "          'Joining Date': last_company_info[2],\n",
        "          'Description': ' '.join(last_company_info[3:])\n",
        "      }\n",
        "      work_history.append(last_work_entry)\n",
        "      parsed_resume['work_history']=work_history\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ddZ9RL2YtMM",
        "outputId": "7c2ece02-e029-4e3f-ac1d-a70bf4023e68"
      },
      "execution_count": 66,
      
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('parsed_resume.json', 'w') as json_file:\n",
        "    json.dump(parsed_resume, json_file, indent=2)\n",
        "\n",
        "print('JSON file saved successfully.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEtPsAEjFDTB",
        "outputId": "d294fe07-a690-498d-c048-70e17713aca6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yD5aWNqxJH6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
